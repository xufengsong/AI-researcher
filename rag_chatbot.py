import streamlit as st
import os
import hashlib
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(".env")
# os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# ë§Œì•½ .envì— OPENAI_API_BASEê°€ ìˆë‹¤ë©´ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°í•´ì•¼ ê³µì‹ APIê°€ í˜¸ì¶œë©ë‹ˆë‹¤.
# os.environ.pop("OPENAI_API_BASE", None) 

# ---- DOC LOAD ----
def load_and_split_docs(uploaded_file):
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë¡œë”ê°€ ì½ì„ ìˆ˜ ìˆê²Œ í•¨
    with open(uploaded_file.name, "wb") as f:
        f.write(uploaded_file.getbuffer())

    if uploaded_file.name.endswith(".pdf"):
        loader = PyPDFLoader(uploaded_file.name)
    else:
        loader = TextLoader(uploaded_file.name, encoding="utf-8")

    documents = loader.load()
    # ì²­í¬ ì‚¬ì´ì¦ˆë¥¼ ì¡°ê¸ˆ ë” í‚¤ìš°ê³  ì˜¤ë²„ë©ì„ ë„‰ë„‰íˆ ì£¼ëŠ” ê²ƒì´ ë¬¸ë§¥ ìœ ì§€ì— ìœ ë¦¬í•  ìˆ˜ ìˆìŒ
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = splitter.split_documents(documents)
    
    # ì²˜ë¦¬ í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ (ì„ íƒ ì‚¬í•­)
    # os.remove(uploaded_file.name)
    
    return docs

# ---- VECTOR STORE ----
def get_vectorstore(docs):
    # text-embedding-3-small ëª¨ë¸ ì‚¬ìš©
    embeddings = OpenAIEmbeddings()
    return FAISS.from_documents(docs, embeddings)

# ---- RAG CHAIN ----
def build_rag_chain(vectordb):
    retriever = vectordb.as_retriever()
    
    prompt = ChatPromptTemplate.from_template("""
    ë„ˆëŠ” ë°˜ë„ì²´ ê¸°ìˆ  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AIì•¼.
    ì•„ë˜ ì§ˆë¬¸ì— ì£¼ì–´ì§„ [ì°¸ê³  ë¬¸ì„œ]ì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µí•´.
    ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•´.

    ì§ˆë¬¸:
    {question}

    [ì°¸ê³  ë¬¸ì„œ]:
    {context}
    """)

    # [ìˆ˜ì •] ëª¨ë¸ëª…ì„ gpt-4.1-mini -> gpt-4o-mini ë¡œ ë³€ê²½
    llm = ChatOpenAI(
        model=" gpt-4.1-mini", 
        temperature=0
    )

    rag_chain = (
        {
            "context": RunnableLambda(lambda x: x["question"]) | retriever,
            "question": RunnableLambda(lambda x: x["question"])
        }
        | prompt
        | llm
    )
    return rag_chain

# ---- UI ----
st.set_page_config(page_title="ë°˜ë„ì²´ ë¬¸ì„œ RAG ì±—ë´‡")
st.title("ğŸ“˜ ë°˜ë„ì²´ ê¸°ìˆ ë¬¸ì„œ RAG ì±—ë´‡")

if "uploaded_file_hash" not in st.session_state:
    st.session_state.uploaded_file_hash = None
if "vectordb" not in st.session_state:
    st.session_state.vectordb = None
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF, TXT)", type=["pdf", "txt"])

def get_file_hash(bytes_data):
    return hashlib.md5(bytes_data).hexdigest()

if uploaded_file:
    file_bytes = uploaded_file.getvalue()
    file_hash = get_file_hash(file_bytes)

    if st.session_state.uploaded_file_hash != file_hash:
        with st.spinner("ë¬¸ì„œ ë¶„ì„ ë° ì„ë² ë”© ìƒì„± ì¤‘..."):
            try:
                split_docs = load_and_split_docs(uploaded_file)
                st.session_state.vectordb = get_vectorstore(split_docs)
                st.session_state.rag_chain = build_rag_chain(st.session_state.vectordb)
                st.session_state.uploaded_file_hash = file_hash
                st.success("ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # íŒŒì¼ì´ ì—…ë¡œë“œë˜ê³  ì²˜ë¦¬ê°€ ì™„ë£Œëœ ìƒíƒœì—ì„œë§Œ ì§ˆë¬¸ ì…ë ¥ì°½ í‘œì‹œ
    if st.session_state.rag_chain:
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

        if question:
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                response = st.session_state.rag_chain.invoke({"question": question})
                st.write(response.content)