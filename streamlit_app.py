import os
import streamlit as st
from dotenv import load_dotenv

load_dotenv(override=True)
load_dotenv("env.txt")  # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (API Key ë“±)

# ë¬¸ì„œ ì²˜ë¦¬ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# ëª¨ë¸ ë° ì„ë² ë”©
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# ì²´ì¸ êµ¬ì„± ìš”ì†Œ
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

from langfuse import Langfuse, get_client
from langfuse.langchain import CallbackHandler

Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_BASE_URL"),
)

langfuse = get_client()

# Replacement for old CallbackHandler()
langfuse_handler = CallbackHandler()



# -------------------------------
# ë¬¸ì„œ ì²˜ë¦¬ ë° ì²´ì¸ êµ¬ì„± (ìºì‹œë¡œ í•œ ë²ˆë§Œ ì‹¤í–‰)
@st.cache_resource(show_spinner="ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
def create_chain():
    # 1. PDF ë¡œë”©
    loader = PyMuPDFLoader("data/ì•„ì´ë””ì–´ ë³´í˜¸ë¥¼ ìœ„í•œ ê°€ì´ë“œë¼ì¸ ê°œì • í•´ì„¤ì„œ.pdf")
    docs = loader.load()

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)
    split_docs = text_splitter.split_documents(docs)

    # 3. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small", openai_api_base=os.environ["EMBED_BASE_URL"]
    )
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    # 4. í”„ë¡¬í”„íŠ¸ ë° ëª¨ë¸
    prompt = ChatPromptTemplate.from_template(
        """
        ë‹¤ìŒì€ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ëœ ì •ë³´ì…ë‹ˆë‹¤:
        {context}

        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µí•˜ë˜, ë°˜ë“œì‹œ ë‹¤ìŒ ì›ì¹™ì„ ì§€í‚¤ì„¸ìš”.
        1. ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        2. ë‹µë³€ì—ëŠ” ë°˜ë“œì‹œ í•´ë‹¹ ë‚´ìš©ì˜ ì¡°í•­, í•­ëª©ê³¼ í˜ì´ì§€ ì •ë³´ë¥¼ í•¨ê»˜ ëª…ì‹œí•˜ì„¸ìš”. (ì˜ˆ: "3.2í•­, p.8")
        3. ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
        4. í—ˆêµ¬ì˜ ê·œì •, ê¸ˆì•¡, ì¡°í•­ë²ˆí˜¸ ë“±ì„ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
        5. ë‹µë³€ì€ ê°„ê²°í•˜ê³ , ë¶ˆí•„ìš”í•œ í•´ì„¤ ì—†ì´ í•œë‘ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.

        ì˜ˆì‹œ ì§ˆë¬¸ : ì™¸ë¶€ì¸ì—ê²Œ ì•„ì´ë””ì–´ë¥¼ ê³µìœ í•´ë„ ë˜ë‚˜ìš”?
        ì˜ˆì‹œ ë‹µë³€ : ê°€ì´ë“œë¼ì¸ 3.4í•­ì— ë”°ë¼, ë¹„ë°€ìœ ì§€ê³„ì•½(NDA) ì²´ê²° í›„ ê³µìœ  ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì¶œì²˜: 3.4í•­, p.9)

        ì§ˆë¬¸: {question}
        """
    )

    llm = ChatOpenAI(model="openai/gpt-4.1-mini", temperature=0)
    parser = StrOutputParser()

    # LCEL ê¸°ë°˜ ì²´ì¸ êµ¬ì„±
    chain = (
        RunnableLambda(
            lambda x: {
                "context": retriever.invoke(x["question"]),
                "question": x["question"],
            }
        )
        | prompt
        | llm
        | parser
    )

    return chain


# -------------------------------
# Streamlit UI êµ¬ì„±
st.set_page_config(
    page_title="ğŸ“„ á„‹á…¡á„‹á…µá„ƒá…µá„‹á…¥ á„‡á…©á„’á…©á„…á…³á†¯ á„‹á…±á„’á…¡á†« á„€á…¡á„‹á…µá„ƒá…³á„…á…¡á„‹á…µá†« ì±—ë´‡"
)
st.title("ğŸ“„ á„‹á…¡á„‹á…µá„ƒá…µá„‹á…¥ á„‡á…©á„’á…© á„€á…¡á„‹á…µá„ƒá…³á„…á…¡á„‹á…µá†« ì±—ë´‡")

# ì²´ì¸ ì´ˆê¸°í™”
chain = create_chain()

# ì…ë ¥ì°½
user_input = st.text_input(
    "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
    placeholder="ì˜ˆ: ì œì•ˆì„œì— ê¸°ë°€ í‘œì‹œë¥¼ ì•ˆ í•˜ë©´ ë³´í˜¸ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
)

# ì‘ë‹µ ì¶œë ¥
if user_input:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            response = chain.invoke(
                {"question": user_input},
                config={"callbacks": [langfuse_handler]}
            )
            st.success(response)
        except Exception as e:
            st.error(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")
